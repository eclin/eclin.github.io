<!DOCTYPE html>

<html>
    <head>
        <title>Math Model</title>

        <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    </head>

    <body>
        <!-- Input field -->
        <p>This is a simple calculator.</p>
        <p>Format: "[num1] [op] [num2]"</p>
        <p>num1, num2 are between -99 and 99</p>
        <p>op is one of: [+, -, *]</p>


        <label for="userInputLocal">Enter Input:</label>
        <input type="text" id="userInputLocal">
        <button id="runModelLocal" onclick="runModel()">Go</button>

        <h2>Local Model Answer:</h2>
        <pre id="localResult"></pre>
        
        <label for="userInputLambda">Enter Input:</label>
        <input type="text" id="userInputLambda">
        <button id="runModelLambda" onclick="callLambda()">Go</button>

        <h2>Server Model Answer:</h2>
        <pre id="lambdaResult"></pre>

        <script>
            document.getElementById("userInputLambda").addEventListener("keydown", function(event) {
                if (event.key === "Enter") { // Check if Enter key is pressed
                    document.getElementById("runModelLambda").click(); // Trigger button click
                }
            });
            document.getElementById("userInputLocal").addEventListener("keydown", function(event) {
                if (event.key === "Enter") { // Check if Enter key is pressed
                    document.getElementById("runModelLocal").click(); // Trigger button click
                }
            });
            async function callLambda() {
                document.getElementById("lambdaResult").textContent = "Calculating ...";
                const apiUrl = "https://rikv8iqlfd.execute-api.us-east-2.amazonaws.com/default/runMathModel";
                
                const requestBody = {
                    equation: document.getElementById("userInputLambda").value
                };

                try {
                    const response = await fetch(apiUrl, {
                        method: "POST",
                        headers: {
                            "Origin": "*",
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify(requestBody)
                    });

                    const data = await response.json();
                    document.getElementById("lambdaResult").textContent = JSON.stringify(data, null, 2);
                } catch (error) {
                    console.error("Error:", error);
                    document.getElementById("lambdaResult").textContent = "Error calling Lambda";
                }
            }
            async function runModel() {
                document.getElementById("localResult").textContent = "Calculating ...";
                try {
                    // Load ONNX model
                    const session = await ort.InferenceSession.create("model.onnx");
                    const TOKENS = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', ' ', '-', '+', '*', '<PAD>', '<BOS>', '<END>']
                    const numTokens = TOKENS.length

                    // Get user input and process it
                    let inputText = document.getElementById("userInputLocal").value;

                    let contextValues = inputText.split("").map(char => TOKENS.indexOf(char))
                    while (contextValues.length < 9) {
                        contextValues.push(TOKENS.indexOf('<PAD>'))
                    }
                    let contextTensor = new ort.Tensor("int64", contextValues, [1, 9]);

                    let primaryValues = [15, 0, 0, 0, 0, 0, 0]
                    let result = ""

                    for (let i = 0; i < 6; i++) {
                        // Create ONNX tensor (adjust dimensions based on your model)
                        let primaryTensor = new ort.Tensor("int64", primaryValues, [1, 7]);
                        // Prepare model input
                        const feeds = { context: contextTensor, primary: primaryTensor };
                        // Run inference
                        const results = await session.run(feeds);
                        // Get the first output (adjust if your model has multiple outputs)
                        let output = results[Object.keys(results)[0]].data;

                        let outputSlice = output.slice(i * numTokens, (i + 1) * numTokens)
                        const argmax = outputSlice.indexOf(Math.max(...outputSlice));

                        outputToken = TOKENS[argmax]
                        primaryValues[i + 1] = argmax
                        result += TOKENS[argmax]
                        document.getElementById("localResult").textContent = result
                    }
            } catch (error) {
                console.error("Error running ONNX model:", error);
                document.getElementById("localResult").textContent = "Error: " + error.message;
            }
        }
    </script>
    </body>
</html>
